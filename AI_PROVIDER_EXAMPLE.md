# ü§ñ Exemplo de Configura√ß√£o de Provedores de IA

Este arquivo demonstra como configurar e usar diferentes provedores de IA no Swagger Agent.

## üîß Configura√ß√£o B√°sica

O Swagger Agent suporta m√∫ltiplos provedores de IA atrav√©s da propriedade `app.ai.provider`. Apenas **um provedor ser√° carregado por vez**.

### Propriedade de Configura√ß√£o

```yaml
# application.yml
app:
  ai:
    provider: ${AI_PROVIDER:openai}  # Valores: "openai" ou "ollama"
```

## üöÄ Exemplo 1: Usando OpenAI

### Configura√ß√£o via Vari√°vel de Ambiente

```bash
# Configure o provedor como OpenAI
export AI_PROVIDER=openai

# Configure sua chave da API OpenAI
export OPENAI_API_KEY="sua_chave_openai_aqui"

# Execute a aplica√ß√£o
mvn spring-boot:run
```

### Configura√ß√£o via application.yml

```yaml
# application.yml
app:
  ai:
    provider: openai

spring:
  ai:
    openai:
      api-key: ${OPENAI_API_KEY}
      chat:
        options:
          model: gpt-4o-mini
          temperature: 0.7
          max-tokens: 2048
```

## üè† Exemplo 2: Usando Ollama

### Pr√©-requisitos

1. **Instale o Ollama:**
```bash
# Com Docker
docker run -d --name ollama -p 11434:11434 ollama/ollama:latest

# Ou instale diretamente: https://ollama.com/download
```

2. **Baixe um modelo:**
```bash
# Exemplo com Qwen 2.5
docker exec -it ollama ollama pull qwen2.5:0.5b

# Ou se instalou diretamente
ollama pull qwen2.5:0.5b
```

### Configura√ß√£o via Vari√°vel de Ambiente

```bash
# Configure o provedor como Ollama
export AI_PROVIDER=ollama

# Configure o Ollama
export SPRING_AI_OLLAMA_BASE_URL=http://localhost:11434
export SPRING_AI_OLLAMA_CHAT_OPTIONS_MODEL="qwen2.5:0.5b"

# Execute a aplica√ß√£o
mvn spring-boot:run
```

### Configura√ß√£o via application.yml

```yaml
# application.yml
app:
  ai:
    provider: ollama

spring:
  ai:
    ollama:
      base-url: ${SPRING_AI_OLLAMA_BASE_URL:http://localhost:11434}
      chat:
        options:
          model: ${SPRING_AI_OLLAMA_CHAT_OPTIONS_MODEL:qwen2.5:0.5b}
          temperature: 0.7
```

## üîÑ Exemplo 3: Troca de Provedores

### De OpenAI para Ollama

```bash
# 1. Pare a aplica√ß√£o (Ctrl+C)

# 2. Configure para usar Ollama
export AI_PROVIDER=ollama
export SPRING_AI_OLLAMA_BASE_URL=http://localhost:11434
export SPRING_AI_OLLAMA_CHAT_OPTIONS_MODEL="qwen2.5:0.5b"

# 3. Reinicie a aplica√ß√£o
mvn spring-boot:run
```

### De Ollama para OpenAI

```bash
# 1. Pare a aplica√ß√£o (Ctrl+C)

# 2. Configure para usar OpenAI
export AI_PROVIDER=openai
export OPENAI_API_KEY="sua_chave_openai_aqui"

# 3. Reinicie a aplica√ß√£o
mvn spring-boot:run
```

## üìã Logs de Inicializa√ß√£o

### Log com OpenAI
```
üîß Configurando OpenAI como provedor de IA
```

### Log com Ollama
```
üîß Configurando Ollama como provedor de IA
```

## ‚ö†Ô∏è Importante

- **Apenas um provedor √© carregado por vez**
- **A aplica√ß√£o deve ser reiniciada ap√≥s alterar o provedor**
- **Verifique os logs para confirmar qual provedor est√° sendo usado**

## üß™ Testando

Ap√≥s configurar, teste a aplica√ß√£o:

1. **Acesse a interface web:** `http://localhost:8080/chat.html`
2. **Fa√ßa uma pergunta:** "Ol√°, como voc√™ est√°?"
3. **Verifique se est√° funcionando** com o provedor configurado

## üîç Troubleshooting

### Erro: "No qualifying bean of type 'ChatModel'"
- Verifique se a propriedade `app.ai.provider` est√° configurada corretamente
- Certifique-se de que as depend√™ncias do provedor est√£o no classpath

### Erro: "Connection refused" (Ollama)
- Verifique se o Ollama est√° rodando: `docker ps` ou `ollama list`
- Confirme a URL: `http://localhost:11434`

### Erro: "Invalid API key" (OpenAI)
- Verifique se a vari√°vel `OPENAI_API_KEY` est√° configurada
- Confirme se a chave √© v√°lida 